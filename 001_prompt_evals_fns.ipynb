{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5437be1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load env variables and create client\n",
    "from dotenv import load_dotenv\n",
    "from anthropic import Anthropic\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = Anthropic()\n",
    "model = \"claude-haiku-4-5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b0d8e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def add_user_message(messages, text):\n",
    "    user_message = {\"role\": \"user\", \"content\": text}\n",
    "    messages.append(user_message)\n",
    "\n",
    "\n",
    "def add_assistant_message(messages, text):\n",
    "    assistant_message = {\"role\": \"assistant\", \"content\": text}\n",
    "    messages.append(assistant_message)\n",
    "\n",
    "\n",
    "def chat(messages, system=None, temperature=1.0, stop_sequences=[]):\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"max_tokens\": 1000,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"stop_sequences\": stop_sequences,\n",
    "    }\n",
    "\n",
    "    if system:\n",
    "        params[\"system\"] = system\n",
    "\n",
    "    message = client.messages.create(**params)\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e788701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a new dataset\n",
    "import json\n",
    "\n",
    "\n",
    "def generate_dataset():\n",
    "    prompt = \"\"\"\n",
    "Generate a evaluation dataset for a prompt evaluation. The dataset will be used to evaluate prompts\n",
    "that generate Python, JSON, or Regex specifically for AWS-related tasks. Generate an array of JSON objects,\n",
    "each representing task that requires Python, JSON, or a Regex to complete.\n",
    "\n",
    "Example output:\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"task\": \"Description of task\",\n",
    "        \"format\": \"json\" or \"python\" or \"regex\",\n",
    "        \"solution_criteria\": \"Key criteria to evaluate the solution\"\n",
    "    },\n",
    "    ...additional\n",
    "]\n",
    "```\n",
    "\n",
    "* Focus on tasks that can be solved by writing a single Python function, a single JSON object, or a regular expression.\n",
    "* Focus on tasks that do not require writing much code\n",
    "\n",
    "Please generate 3 objects.\n",
    "\"\"\"\n",
    "\n",
    "    messages = []\n",
    "    add_user_message(messages, prompt)\n",
    "    add_assistant_message(messages, \"```json\")\n",
    "    text = chat(messages, stop_sequences=[\"```\"])\n",
    "    return json.loads(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "438ed743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the dataset and write it to 'dataset.json'\n",
    "dataset = generate_dataset()\n",
    "with open(\"dataset.json\", \"w\") as f:\n",
    "    json.dump(dataset, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36b89174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to grade a test case + output using a model\n",
    "def grade_by_model(test_case, output):\n",
    "    eval_prompt = f\"\"\"\n",
    "You are an expert AWS code reviewer. Your task is to evaluate the following AI-generated solution.\n",
    "\n",
    "Original Task:\n",
    "<task>\n",
    "{test_case[\"task\"]}\n",
    "</task>\n",
    "\n",
    "Solution to Evaluate:\n",
    "<solution>\n",
    "{output}\n",
    "</solution>\n",
    "\n",
    "Criteria you should use to evaluate the solution:\n",
    "<criteria>\n",
    "{test_case[\"solution_criteria\"]}\n",
    "</criteria>\n",
    "\n",
    "Output Format\n",
    "Provide your evaluation as a structured JSON object with the following fields, in this specific order:\n",
    "- \"strengths\": An array of 1-3 key strengths\n",
    "- \"weaknesses\": An array of 1-3 key areas for improvement\n",
    "- \"reasoning\": A concise explanation of your overall assessment\n",
    "- \"score\": A number between 1-10\n",
    "\n",
    "Respond with JSON. Keep your response concise and direct.\n",
    "Example response shape:\n",
    "{{\n",
    "    \"strengths\": string[],\n",
    "    \"weaknesses\": string[],\n",
    "    \"reasoning\": string,\n",
    "    \"score\": number\n",
    "}}\n",
    "    \"\"\"\n",
    "\n",
    "    messages = []\n",
    "    add_user_message(messages, eval_prompt)\n",
    "    add_assistant_message(messages, \"```json\")\n",
    "    eval_text = chat(messages, stop_sequences=[\"```\"])\n",
    "    return json.loads(eval_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83809a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passes a test case into Claude\n",
    "def run_prompt(test_case):\n",
    "    prompt = f\"\"\"\n",
    "Please solve the following task:\n",
    "\n",
    "{test_case[\"task\"]}\n",
    "\n",
    "* Respond only with {test_case[\"format\"]}\n",
    "* Do not add any comments or commentary or explanation\n",
    "\"\"\"\n",
    "\n",
    "    messages = []\n",
    "    add_user_message(messages, prompt)\n",
    "    add_assistant_message(messages, \"```code\")\n",
    "    output = chat(messages, stop_sequences=[\"```\"])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7953c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to validate the output structure\n",
    "import re\n",
    "import ast\n",
    "\n",
    "\n",
    "def validate_json(text):\n",
    "    try:\n",
    "        json.loads(text.strip())\n",
    "        return 10\n",
    "    except json.JSONDecodeError:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def validate_python(text):\n",
    "    try:\n",
    "        ast.parse(text.strip())\n",
    "        return 10\n",
    "    except SyntaxError:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def validate_regex(text):\n",
    "    try:\n",
    "        re.compile(text.strip())\n",
    "        return 10\n",
    "    except re.error:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def grade_syntax(response, test_case):\n",
    "    format = test_case[\"format\"]\n",
    "    if format == \"json\":\n",
    "        return validate_json(response)\n",
    "    elif format == \"python\":\n",
    "        return validate_python(response)\n",
    "    else:\n",
    "        return validate_regex(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9bcc4671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to execute a single test case and grade the output\n",
    "def run_test_case(test_case):\n",
    "    \"\"\"Calls run_prompt, then grades the result\"\"\"\n",
    "    output = run_prompt(test_case)\n",
    "\n",
    "    model_grade = grade_by_model(test_case, output)\n",
    "    model_score = model_grade[\"score\"]\n",
    "    reasoning = model_grade[\"reasoning\"]\n",
    "\n",
    "    syntax_score = grade_syntax(output, test_case)\n",
    "\n",
    "    score = (model_score + syntax_score) / 2\n",
    "\n",
    "    return {\n",
    "        \"output\": output,\n",
    "        \"test_case\": test_case,\n",
    "        \"model_score\": model_score,\n",
    "        \"syntax_score\": syntax_score,\n",
    "        \"score\": score,\n",
    "        \"reasoning\": reasoning,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5fa99d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "\n",
    "def run_eval(dataset):\n",
    "    \"\"\"Loads the dataset and calls run_test_case with each case\"\"\"\n",
    "    results = []\n",
    "\n",
    "    for test_case in dataset:\n",
    "        result = run_test_case(test_case)\n",
    "        results.append(result)\n",
    "\n",
    "    average_score = mean([result[\"score\"] for result in results])\n",
    "    print(f\"Average score: {average_score}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "30fae983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score: 7.833333333333333\n"
     ]
    }
   ],
   "source": [
    "with open(\"dataset.json\", \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "results = run_eval(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dbcc6111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"output\": \"\\narn:aws:iam::\\\\d{12}:role/[a-zA-Z0-9\\\\-_/.]+\\n\",\n",
      "    \"test_case\": {\n",
      "      \"task\": \"Extract all AWS IAM role ARNs from a CloudFormation template output string\",\n",
      "      \"format\": \"regex\",\n",
      "      \"solution_criteria\": \"Regex should match ARN format: arn:aws:iam::[0-9]{12}:role/[a-zA-Z0-9-_/]+ and correctly capture only role ARNs, not other resource types\"\n",
      "    },\n",
      "    \"model_score\": 7,\n",
      "    \"syntax_score\": 10,\n",
      "    \"score\": 8.5,\n",
      "    \"reasoning\": \"The regex pattern closely matches the required format and will successfully identify AWS IAM role ARNs. However, the solution lacks the + quantifier after the character class (should be [a-zA-Z0-9\\\\-_/.]+, not [a-zA-Z0-9\\\\-_/.]+ without the +), which would allow matching role names with zero characters. Additionally, the pattern lacks anchors or delimiters to prevent false matches in contexts where the pattern appears as a substring. For practical extraction in CloudFormation output, capture groups would be beneficial.\"\n",
      "  },\n",
      "  {\n",
      "    \"output\": \"\\n{\\n  \\\"Version\\\": \\\"2012-10-17\\\",\\n  \\\"Statement\\\": [\\n    {\\n      \\\"Sid\\\": \\\"ListAndDescribeProductionInstances\\\",\\n      \\\"Effect\\\": \\\"Allow\\\",\\n      \\\"Action\\\": [\\n        \\\"ec2:DescribeInstances\\\",\\n        \\\"ec2:DescribeTags\\\"\\n      ],\\n      \\\"Resource\\\": \\\"*\\\",\\n      \\\"Condition\\\": {\\n        \\\"StringEquals\\\": {\\n          \\\"ec2:ResourceTag/Environment\\\": \\\"Production\\\"\\n        }\\n      }\\n    },\\n    {\\n      \\\"Sid\\\": \\\"AllowDescribeInstancesForTagFiltering\\\",\\n      \\\"Effect\\\": \\\"Allow\\\",\\n      \\\"Action\\\": [\\n        \\\"ec2:DescribeInstances\\\"\\n      ],\\n      \\\"Resource\\\": \\\"*\\\"\\n    }\\n  ]\\n}\\n\",\n",
      "    \"test_case\": {\n",
      "      \"task\": \"Create a JSON policy document that allows an IAM user to list and describe only EC2 instances with a specific tag 'Environment=Production'\",\n",
      "      \"format\": \"json\",\n",
      "      \"solution_criteria\": \"JSON should be valid IAM policy syntax with appropriate Action, Resource, and Condition elements that restrict access to Production-tagged instances\"\n",
      "    },\n",
      "    \"model_score\": 3,\n",
      "    \"syntax_score\": 10,\n",
      "    \"score\": 6.5,\n",
      "    \"reasoning\": \"While the policy has valid syntax and structure, it fails the core requirement of restricting access to only Production-tagged instances. The second statement creates a critical security flaw by allowing unrestricted DescribeInstances access. Additionally, EC2 tag-based conditions don't filter API response results for DescribeInstances - they only control whether actions can be performed on resources. A proper solution would require either application-level filtering or use of resource ARNs if available, with a single statement that doesn't grant blanket permissions.\"\n",
      "  },\n",
      "  {\n",
      "    \"output\": \"\\nimport re\\n\\ndef is_valid_s3_bucket_name(bucket_name: str) -> bool:\\n    \\\"\\\"\\\"\\n    Validates an AWS S3 bucket name according to AWS naming rules.\\n    \\n    Rules:\\n    - 3 to 63 characters long\\n    - Can consist only of lowercase letters, numbers, and hyphens (-)\\n    - Must start with a lowercase letter or number\\n    - Must end with a lowercase letter or number\\n    - Cannot have consecutive hyphens\\n    - Cannot be formatted as an IP address (e.g., 192.168.1.1)\\n    \\\"\\\"\\\"\\n    if not isinstance(bucket_name, str):\\n        return False\\n    \\n    # Check length\\n    if len(bucket_name) < 3 or len(bucket_name) > 63:\\n        return False\\n    \\n    # Check if it matches the pattern: lowercase letters, numbers, and hyphens\\n    if not re.match(r'^[a-z0-9][a-z0-9\\\\-]*[a-z0-9]$|^[a-z0-9]$', bucket_name):\\n        return False\\n    \\n    # Check for consecutive hyphens\\n    if '--' in bucket_name:\\n        return False\\n    \\n    # Check if it's formatted as an IP address\\n    if re.match(r'^(\\\\d+\\\\.)+\\\\d+$', bucket_name):\\n        return False\\n    \\n    return True\\n\\n\\n# Test cases\\nif __name__ == \\\"__main__\\\":\\n    test_cases = [\\n        (\\\"my-bucket\\\", True),\\n        (\\\"mybucket\\\", True),\\n        (\\\"my-bucket-123\\\", True),\\n        (\\\"a\\\", False),  # Too short\\n        (\\\"ab\\\", False),  # Too short\\n        (\\\"abc\\\", True),  # Valid minimum\\n        (\\\"my--bucket\\\", False),  # Consecutive hyphens\\n        (\\\"My-bucket\\\", False),  # Uppercase\\n        (\\\"my-bucket-\\\", False),  # Ends with hyphen\\n        (\\\"-my-bucket\\\", False),  # Starts with hyphen\\n        (\\\"my_bucket\\\", False),  # Contains underscore\\n        (\\\"my.bucket\\\", False),  # Contains dot\\n        (\\\"123.456.789.012\\\", False),  # IP address format\\n        (\\\"my-bucket-\\\" + \\\"a\\\" * 50, False),  # Too long\\n        (\\\"123\\\", True),  # Valid - only numbers\\n        (\\\"1-2-3\\\", True),  # Valid - numbers with hyphens\\n    ]\\n    \\n    for bucket_name, expected in test_cases:\\n        result = is_valid_s3_bucket_name(bucket_name)\\n        status = \\\"\\u2713\\\" if result == expected else \\\"\\u2717\\\"\\n        print(f\\\"{status} {bucket_name!r}: {result} (expected {expected})\\\")\\n\",\n",
      "    \"test_case\": {\n",
      "      \"task\": \"Write a Python function that takes an AWS S3 bucket name and returns True if the bucket name is valid according to AWS naming rules (3-63 characters, lowercase, no consecutive hyphens, etc.)\",\n",
      "      \"format\": \"python\",\n",
      "      \"solution_criteria\": \"Function should validate all S3 bucket naming constraints and return boolean; should handle edge cases like consecutive hyphens, uppercase letters, and length boundaries\"\n",
      "    },\n",
      "    \"model_score\": 7,\n",
      "    \"syntax_score\": 10,\n",
      "    \"score\": 8.5,\n",
      "    \"reasoning\": \"The solution correctly implements the core AWS S3 bucket naming rules and handles most critical edge cases with good test coverage. The function is defensive and well-documented. However, the regex patterns are unnecessarily complex and the IP address validation is incomplete. The logic itself works correctly for the primary use cases, but the IP check wouldn't catch all invalid patterns AWS might reject. Overall, this is a solid, functional solution with room for improvement in code clarity and edge case handling.\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(results, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
